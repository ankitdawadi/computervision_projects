{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Activation,Dense,Dropout\n",
    "from keras import backend as k\n",
    "#from keras.application import \n",
    "\n",
    "train_path=\"G:/Datasets/dogs-cats-images/dataset/training_set\"\n",
    "validating_path=\"G:/Datasets/dogs-cats-images/dataset/test_set\"\n",
    "test_path=\"G:/Datasets/extra_datasets/Test_Set\"\n",
    "img_width,img_height=32,32\n",
    "epochs=2\n",
    "batch_size=1\n",
    "channels=3\n",
    "input_shape=(img_width,img_height,3)\n",
    "datagen=ImageDataGenerator(rescale=1./255,rotation_range=5,zoom_range=0.2,horizontal_flip=True)\n",
    "validate_datagen=ImageDataGenerator(rescale=1./255)\n",
    "train_generator=datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_width,img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode= None,\n",
    "    color_mode='rgb')\n",
    "validation_generator=validate_datagen.flow_from_directory(validating_path,\n",
    "                                                          target_size=(img_width,img_height),\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          class_mode='binary')\n",
    "i=0;\n",
    "list=[]\n",
    "for image in train_generator:\n",
    "    list.append(image)\n",
    "    i=i+1\n",
    "    if i>5:\n",
    "        break\n",
    "for image in list:\n",
    "    plt.imshow(np.squeeze(image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAFhCAYAAAAiOyLsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABthJREFUeJzt3cGLr1UZwPHn1C2jaGF7K6MoDLJlJLi4G1uErcJVhEKLVq5qFS3MXeB/IC4iMIgWgota1EpqFQpdwTTu0p2iVFIGp8XMwEUH/eV37vV3m89n+b7zzJyB4cvhnfPOrL33APDBfeTDXgDA7U5IASIhBYiEFCASUoBISAEiIeWorbXuWmv9eq31xlrrzbXWb9Zanz1w9hNrrZ+vtV5da7211vrjWuv+m71mLh8h5WittT45M7+fma/MzPdn5nsz86WZ+cNa61MHfIonZ+YHM/PTmfn2zLw6M79da3395qyYy2o5kM+xWms9OjNPzMyX996vnF67e2Zenpkf772feI/Ze2fm+Zl5ZO/91Om1KzNzbWZe2ns/eLPXz+VhR8oxe3Bm/nQW0ZmZvff1mXluZr5zwOzbM/OrG2b/MzNPz8wDa607Ln65XFZCyjH76sz85Zzr12bmngNmr++9/3nO7Mdn5ot9eXBCSDlmn5mZ18+5/trM3Blmz+7DhRBSjt15D/HXAXMrzML/REg5Zq/P+TvHO+f83eaNXnuP2bP7cCGElGN2bU6edb7TPTPz4gGzd58eoXrn7L9n5pV3j8AHI6Qcs2dm5htrrS+cXVhrfX5m7ju9936zH5uZ794we2VmHpqZ3+29/3XRi+Xyco6Uo3V66P6FmXlrZn4yJ888fzYzn56Zr+29/376cZ+bmb/NzGN778dumH96Zh6YmR/NzPWZ+eGcHMz/5t77z7fwW+H/nB0pR2vv/Y+ZuTozf52ZX8zML+ckiFfPInpqzcxH590/zw/PzFMz8/jMPDszd83Mt0SUi2ZHChDZkQJEQgoQCSlAJKQA0ZVb+cXWWn6zBdy29t7nvmJsRwoQCSlAJKQAkZACREIKEAkpQCSkAJGQAkRCChDd0jeb4LJah/zPvQNf/POXL4+PHSlAJKQAkZACREIKEAkpQCSkAJGQAkRCChA5kA+3wJ4DTtE7aH/bsiMFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWI1t77w14DwG3NjhQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAEiIQWIhBQgElKASEgBIiEFiIQUIBJSgEhIASIhBYiEFCASUoBISAGi/wIxDpLWMLlzGQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plots(ims, figsize=(12,6), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(rows, cols, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=16)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "imgs, labels=next(validation_generator)\n",
    "plots(imgs, titles=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=Sequential()\n",
    "#model.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(227,227,3),activation='relu'))\n",
    "#model.add(MaxPooling2D((3,3),strides=(2,2)))\n",
    "#model.add(Conv2D(256,(5,5),padding='same',activation='relu'))\n",
    "#model.add(MaxPooling2D((3,3),strides=(2,2)))\n",
    "#model.add(Conv2D(384,(3,3),padding='same',activation='relu'))\n",
    "#model.add(Conv2D(384,(3,3),padding='same',activation='relu'))\n",
    "#model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "#model.add(MaxPooling2D((3,3),strides=(2,2)))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(9216,activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(4096,activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(4096,activation='tanh'))\n",
    "#model.add(Dense(2,activation='softmax'))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(6,kernel_size=(5,5),strides=(1,1),padding='valid',input_shape=(img_height,img_width,channels)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(AveragePooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(16,kernel_size=(5,5),padding='valid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "    \n",
    "#model.add(Conv2D(384,kernel_size=(3,3),padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "#model.add(Conv2D(384,kernel_size=(3,3),padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "#model.add(Conv2D(256,kernel_size=(3,3),padding='same'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu')) \n",
    "#model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(2048,activation='tanh'))\n",
    "\n",
    "model.add(Dense(120,activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(84,activation='tanh'))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(4096,activation='tanh'))\n",
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 6)         456       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 28, 28, 6)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 10, 10, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 10, 10, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 85        \n",
      "=================================================================\n",
      "Total params: 61,305\n",
      "Trainable params: 61,273\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 254s 5s/step - loss: 0.6715 - acc: 0.5816 - val_loss: 0.7579 - val_acc: 0.4797\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 84s 2s/step - loss: 0.6181 - acc: 0.6581 - val_loss: 0.6082 - val_acc: 0.6631\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 40s 791ms/step - loss: 0.6000 - acc: 0.6674 - val_loss: 0.8793 - val_acc: 0.4766\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 39s 785ms/step - loss: 0.5887 - acc: 0.6880 - val_loss: 0.6957 - val_acc: 0.5601\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 40s 793ms/step - loss: 0.5786 - acc: 0.6889 - val_loss: 0.6497 - val_acc: 0.6226\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 39s 787ms/step - loss: 0.5713 - acc: 0.6961 - val_loss: 0.7234 - val_acc: 0.5516\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 39s 785ms/step - loss: 0.5675 - acc: 0.6987 - val_loss: 0.6563 - val_acc: 0.6153\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 39s 789ms/step - loss: 0.5571 - acc: 0.7123 - val_loss: 0.4499 - val_acc: 0.7971\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 39s 779ms/step - loss: 0.5490 - acc: 0.7166 - val_loss: 0.6087 - val_acc: 0.6758\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 39s 787ms/step - loss: 0.5511 - acc: 0.7161 - val_loss: 0.6443 - val_acc: 0.6494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af3b3a5748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=128\n",
    "train_generator=datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(img_width,img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb')\n",
    "\n",
    "test_generator=validate_datagen.flow_from_directory(validating_path,\n",
    "                                                    color_mode='rgb',\n",
    "                                                    target_size=(img_height,img_width),\n",
    "                                                    batch_size=batch_size,\n",
    "                                                    class_mode='binary',\n",
    "                                                    shuffle=False\n",
    "                                                    \n",
    "                                                    \n",
    ")\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=50,\n",
    "                    validation_steps=10,\n",
    "                    validation_data=test_generator,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "img=\"D:/Try/dog.jpg\"\n",
    "\n",
    "#for img in os.listdir(path):\n",
    "imge = cv2.imread(img)\n",
    "img1=cv2.resize(imge, (32,32))\n",
    "plt.imshow(img1,cmap=\"gray\")\n",
    "        \n",
    "plt.show()\n",
    "        #x = image.img_to_array(img1)\n",
    "       \n",
    "        #x = np.expand_dims(x, axis=0)\n",
    "        #print(model.predict(x))\n",
    "        \n",
    "\n",
    "\n",
    "x = image.img_to_array(img1)\n",
    "x/=255\n",
    "\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
